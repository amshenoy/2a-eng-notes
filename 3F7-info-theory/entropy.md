# Entropy

$$ H(X) = \sum_{x} P(x) log \Big( \dfrac{1}{P(x)} \Big) $$

### Binary Entropy Function
$$ 
H_{2}(p) = p log(\dfrac{1}{p}) + (1-p)log(\dfrac{1}{1-p})
$$

## Properties of Entropy

### 1) $ H(X) \ge 0 $
### 2) $ H(X) \le log(M) $ where $ M $ is the alphabet size
### 3) Discrete Uniform Distribution with $ p = \dfrac{1}{M} $ has maximum entropy with a value of $ log(M) $

## Joint & Conditional Entropy

