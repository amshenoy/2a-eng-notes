# Linear Algebra



</br><hr>

## Hermitian Operator

Dot Product (Complex Case) &emsp; $ \large  \mathbf{x} \cdot \mathbf{y} = \mathbf{x}^{\mathbf{H}} \mathbf{y} = ( \mathbf{y}^{\mathbf{H}} \mathbf{x} )^{*}$

Note: In the complex case, $ \mathbf{x} \cdot \mathbf{y} \ne \mathbf{y} \cdot \mathbf{x} $

</br>

## Hermitian Matrix

A matrix $ A $ is Hermitian if $ \large A = A^{*T} $ ie. $ \large A^{\textbf{H}} = A $.

Hermitian matrix has **real eigenvalues** and **orthognal eigenvectors** $ \mathbf{u_{i}}^{\mathbf{H}} \mathbf{u_{j}} = 0 $

## Unitary Matrix


## Positive Definite Matrix


</br><hr>

# Norms

$$ \Large I_{p} \enspace \text{Norm} = \lVert x \rVert_{p} = \Big( \sum_{i=1}^{n} |x_{i}|^{p} \Big)^{\dfrac{1}{p}} $$

$ \Large \lVert x \rVert_{1} = \sum_{i}|x_{i}| $

$ \Large \lVert x \rVert_{2} = \sum_{i} x_{i}^{2} \qquad \Large \lVert x \rVert_{2}^{2} = \mathbf{x}^{\mathbf{H}}  \mathbf{x} $

$ \Large \lVert x \rVert_{\infty} = \max_{i}(x_{i}) $



$ \Large \lVert x \rVert_{\mathbf{A}} = \mathbf{x}^{\mathbf{H}} A \ \mathbf{x} $ &emsp; &emsp; (**Matrix Norm**) 

## Frobenius Norm


</br><hr>

# Least Squares Fitting




</br><hr>

# Singular Value Decomposition






