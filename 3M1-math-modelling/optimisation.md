# Optimisation

# Introduction

### Hessian Matrix

$$ \nabla(\nabla f(x)) = H(f(x)) $$

$$ H_{i,j}(f(x)) = \dfrac{\partial f^{2}}{\partial x_{i} \partial x_{j}} $$

### Stationary Point
$ \nabla f(x^{*}) = 0 $

$ x^{*} $ is either a minimum, maximum or saddle.

#### Minima Types

**Global** - Global minimum (with other possible global minima) </br>
**Strong Global** - Single global minimum </br>
**Weak Local** - Local minimum (with other possible local minima, ie. a plateau) </br>
**Strong Local** - Singular local minimum </br>

## Constraints
- **Equality constraints** can sometimes be eliminated by substitution
- **Inequality constraints** can sometimes be left out and candidate results checked (we will treat them formally with KKT multipliers)

In general, constrained optimization is more difficult to solve than unconstrained optimization.


## Modality


## Convexity


</br><hr></br>

# Unconstrained Optimisation

## Line search
Gradient methods


</br><hr></br>

# Constrained Optimisation

## Linear programming: Simplex Algorithm
## Lagrange and Karush-Kuhn-Tucker (KKT) multipliers
Note that the Karush-Kuhn-Tucker (KKT) multipliers (Kuhn-Tucker (KT) multipliers)

## Barrier and penalty methods

</br><hr></br>

# Simulated annealing (Global)













